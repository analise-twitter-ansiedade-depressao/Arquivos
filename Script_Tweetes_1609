ANÁLISE DE DADOS DO TWITTER

setwd("C:\\Users\\luciana.lacerda\\Desktop\\Luciana\\Projeto\\Tweetes")
getwd()


install.packages("devtools")
library(devtools)

# Caso nao tenha o devtools (Rtools)
# No Windows -> Baixe e instale a versÃ£o .exe do Rtools no link http://cran.r-project.org/bin/windows/Rtools/.
# Configurar nas variÃ¡veis de ambiente, o PATH para o diretÃ³rio da instalaÃ§Ã£o, Ex: C:/Rtools/
install.packages("pkgbuild") # pkgbuild is not available (for R version 3.5.0)
install.packages("devtools") # make sure you have the latest version from CRAN
library(devtools) # load package
devtools::install_github("r-lib/pkgbuild") # install updated version of pkgbuild from GitHub
library(pkgbuild) # load package
find_rtools() # should be TRUE, assuming you have Rtools 3.5
# No MAC OS -> Baixe e instale o Xcode command line tools
# No Linux ->  Instalar o R development package, chamado de r-devel ou r-base-dev
install.packages("sentiment")

library(sentiment)

if(!is.element("devtools", installed.packages()[, 1]))
  install.packages("devtools", repos = 'http://cran.us.r-project.org')
require(devtools)
if(!is.element("RCurl", installed.packages()[, 1]))
  install.packages("RCurl", repos = 'http://cran.us.r-project.org')
if(!require(Rstem)) install_url("http://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz")
if(!require(sentiment)) install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
if(!is.element("rtweet", installed.packages()[,1]))
  install.packages("rtweet", repos = 'http://cran.us.r-project.org')



install.packages("rtweet")
install.packages("tidyverse")
install.packages("tidytext")
install.packages("magrittr")
install.packages("lubridate")
install.packages("stringr")
install.packages("ggExtra")
install.packages("ggplot2")
install.packages("wordcloud")
install.packages("gridExtra")
install.packages("plotly")
install.packages("RColorBrewer")
install.packages("widyr")
install.packages("igraph")
install.packages("ggraph")
install.packages("sentimentr")
install.packages("lexiconPT")
install.packages("plyr")
install.packages("Rcpp")


library(rtweet)
library(tidyverse) 
library(tidytext)
library(magrittr)  
library(lubridate)
library(stringr) 
library(ggExtra)
library(ggplot2)
library(wordcloud)
library(gridExtra)
library(plotly)
library(RColorBrewer)
library(widyr)
library(igraph)
library(ggraph)
library(plyr)
library(sentimentr)
library(Rcpp)


library(lexiconPT)
ls("package:lexiconPT")

# Obter token e chave do twitter no site apps.twitter.com
api_key             <- "YOUR API KEY"
api_secret          <- "YOUR API SECRET"
access_token        <- "YOUR ACESS TOKEN"
access_token_secret <- "YOUR ACESS TOKEN SECRET"
twitter_app         <- "Twitter Sentiment Analysis."

# Accedemos a Twitter a travÃ©s de los datos del token
create_token(
  app             = twitter_app,
  consumer_key    = api_key,
  consumer_secret = api_secret,
  access_token    = access_token,
  access_secret   = access_token_secret)

# Defininido o volume de tweets e palavras que serão coletadas

tweets_tansiedade    <- search_tweets("transtorno de ansiedade"   , n = 500, include_rts = FALSE, lang="pt")
tweets_ansiedade    <- search_tweets("ansiedade"   , n = 40000, include_rts = FALSE, retryonratelimit = TRUE, lang="pt")

View(tweets_tansiedade)
View(tweets_ansiedade)

# Empilhando as duas bases de coleta em uma só. 

result <- rbind(tweets_tansiedade , tweets_ansiedade)

# criando um vetor com o nome das variáveis que eu quero
result1<-c("user_id","status_id","created_at","screen_name","text","source","display_text_width","is_quote","favorite_count",
  "retweet_count","hashtags","mentions_user_id","mentions_screen_name","geo_coords","coords_coords","bbox_coords",
  "name","location","protected","followers_count","friends_count","listed_count","statuses_count","favorite_count",
  "account_created_at","verified","place_type","place_name","place_full_name")



# Aqui minha base receberá apenas as colunas que foram nomeadas no vetor acima.
resultados=result[result1]

# DEPOIS QUE EMPILHAR TODAS AS BASES DE COLETA:
# criando uma coluna de classificaÃ§Ã£o a partir da palavra buscada no tweet.
result$classificacao = str_extract(result$text, "transtorno de ansiedade")
result$classificacao[is.na(result$classificacao)] <- "ansiedade"


#str(df_resultados)
#sum(is.na(df_resultados))

# total de linhas
n = nrow(resultados)

# porcentagem de NA por coluna
round(colSums(is.na(resultados))*100/n, 2)

# SALVANDO ARQUIVOS PARA JSON

install.packages("jsonlite")
library(jsonlite)


write_json(resultados, "coleta_ansiedade2008_.json")
